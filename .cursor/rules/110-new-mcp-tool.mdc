---
description: Folow these instructions whenever creating new MCP tool functions or modifying existing ones
globs: 
alwaysApply: false
---
# Adding Tools to the MCP Server

## Setting Up New API Endpoints

If your new tool needs to access an API endpoint that is different from the existing Blockscout (dynamically resolved via `get_blockscout_base_url`), BENS (`bens_url`), or Chainscout (`chainscout_url`) endpoints, follow these steps first:

1. **Add endpoint configuration to `blockscout_mcp_server/config.py`**:

   ```python
   class ServerConfig(BaseSettings):
       # Existing endpoints (examples)
       bs_api_key: str = ""
       bs_timeout: float = 120.0
       bens_url: str = "https://bens.services.blockscout.com"
       bens_timeout: float = 30.0
       chainscout_url: str = "https://chains.blockscout.com"
       chainscout_timeout: float = 15.0
       metadata_url: str = "https://metadata.services.blockscout.com"
       metadata_timeout: float = 30.0
       chain_cache_ttl_seconds: int = 1800
       
       # Add your new endpoint
       new_api_url: str = "https://api.example.com"
       new_api_timeout: float = 60.0
       # Add API key if needed
       new_api_key: str = ""
   ```

2. **Create a request helper function in `blockscout_mcp_server/tools/common.py`**:

   ```python
   async def make_new_api_request(api_path: str, params: dict | None = None) -> dict:
       """
       Make a GET request to the New API.
       
       Args:
           api_path: The API path to request
           params: Optional query parameters
           
       Returns:
           The JSON response as a dictionary
           
       Raises:
           httpx.HTTPStatusError: If the HTTP request returns an error status code
           httpx.TimeoutException: If the request times out
       """
       async with httpx.AsyncClient(timeout=config.new_api_timeout) as client:
           if params is None:
               params = {}
           if config.new_api_key:
               params["apikey"] = config.new_api_key  # Adjust based on API requirements

           url = f"{config.new_api_url}{api_path}"
           response = await client.get(url, params=params)
           response.raise_for_status()
           return response.json()
   ```

3. **Update environment configuration files**:
   - Add to `.env.example`:

     ```shell
     BLOCKSCOUT_NEW_API_URL="https://api.example.com"
     BLOCKSCOUT_NEW_API_KEY=""
     BLOCKSCOUT_NEW_API_TIMEOUT=60.0
     ```

   - Add to `Dockerfile`:

     ```dockerfile
     # Existing environment variables
     ENV BLOCKSCOUT_BS_TIMEOUT="120.0"
     ENV BLOCKSCOUT_BENS_URL="https://bens.services.blockscout.com"
     ENV BLOCKSCOUT_METADATA_URL="https://metadata.services.blockscout.com"
     ENV BLOCKSCOUT_METADATA_TIMEOUT="30.0"
     
     # New environment variables
     ENV BLOCKSCOUT_NEW_API_URL="https://api.example.com"
     ENV BLOCKSCOUT_NEW_API_KEY=""
     ENV BLOCKSCOUT_NEW_API_TIMEOUT="60.0"
     ```

## Required File Modifications

When adding a new tool to the MCP server, you need to modify these files:

1. **Create or modify a tool module file** in `blockscout_mcp_server/tools/`:
   - Choose an existing module (e.g., `block_tools.py`, `address_tools.py`) if your tool fits with existing functionality
   - Create a new module if your tool introduces a new category of functionality

2. **Register the tool in `blockscout_mcp_server/server.py`**:
   - Import the tool function
   - Register it with the MCP server using the `@mcp.tool()` decorator

3. **Update documentation in `AGENTS.md`**:
   - If you created a new module, add it to the project structure file listing
   - Add it both in the directory tree structure AND in the "Individual Tool Modules" examples section
   - If you added a tool to an existing module, update the "Examples" section to include your new tool function

## Tool Function Structure

For tools that query Blockscout API (which now support dynamic chain resolution):

```python
from typing import Annotated, Optional
from pydantic import Field
from blockscout_mcp_server.tools.common import make_blockscout_request, get_blockscout_base_url

async def tool_name(
    chain_id: Annotated[str, Field(description="The ID of the blockchain")],
    required_arg: Annotated[str, Field(description="Description of required argument")],
    optional_arg: Annotated[Optional[str], Field(description="Description of optional argument")] = None
) -> dict | list[dict] | str:  # Return type depends on response format
    """
    Descriptive docstring explaining what the tool does.
    Include use cases and examples if helpful.
    """
    # Construct API path, often with argument interpolation
    api_path = f"/api/v2/some_endpoint/{required_arg}"
    
    # Build query parameters for arguments with position: query
    query_params = {}
    if optional_arg:
        query_params["optional_param"] = optional_arg
    
    # Get the Blockscout base URL for the specified chain
    base_url = await get_blockscout_base_url(chain_id)
    
    # Make API request with the dynamic base URL
    # For multiple data sources, consider Performance Optimization patterns (concurrent API calls)
    response_data = await make_blockscout_request(base_url=base_url, api_path=api_path, params=query_params)
    
    # Process response based on implementation patterns below
    # (See Response Handling and Progress Reporting patterns)
    
    return processed_result
```

For tools that use fixed API endpoints (like BENS or other services):

```python
from typing import Annotated, Optional
from pydantic import Field
from blockscout_mcp_server.tools.common import make_bens_request  # or other API helper

async def tool_name(
    required_arg: Annotated[str, Field(description="Description of required argument")],
    optional_arg: Annotated[Optional[str], Field(description="Description of optional argument")] = None
) -> dict | list[dict] | str:  # Return type depends on response format
    """
    Descriptive docstring explaining what the tool does.
    Include use cases and examples if helpful.
    """
    # Construct API path, often with argument interpolation
    api_path = f"/api/v1/some_endpoint/{required_arg}"
    
    # Build query parameters for arguments with position: query
    query_params = {}
    if optional_arg:
        query_params["optional_param"] = optional_arg
    
    # Make API request  
    # For multiple data sources, consider Performance Optimization patterns (concurrent API calls)
    response_data = await make_bens_request(api_path=api_path, params=query_params)
    
    # Process response based on implementation patterns below
    # (See Response Handling and Progress Reporting patterns)
    
    return processed_result
```

## Implementation Patterns

### Response Handling

#### 1. Standardized Structured Responses

**All tools MUST return a standardized `ToolResponse` object.** This ensures consistency and provides clear, machine-readable data to the AI. Do not return raw strings or simple dictionaries.

The `ToolResponse` model separates the primary `data` from metadata like `notes`, `pagination`, and `data_description`.

**Implementation Pattern:**

Use the `build_tool_response` helper from `tools/common.py` to construct the response.

```python
from blockscout_mcp_server.tools.common import build_tool_response
from blockscout_mcp_server.models import PaginationInfo, NextCallInfo

async def some_tool(chain_id: str, address: str, ...) -> dict:  # The SDK will serialize the Pydantic model to a dict
    """A tool demonstrating the correct response format."""
    # 1. Get raw data from an API call
    response_data = await make_blockscout_request(...)

    # 2. Prepare response components
    main_data = response_data.get("items", [])
    notes = None
    pagination = None

    # 3. Handle notes for truncated data
    if response_data.get("was_truncated"):
        notes = ["Some data was truncated. To get the full data, use `...`"]

    # 4. Handle pagination
    next_page_params = response_data.get("next_page_params")
    if next_page_params:
        next_cursor = encode_cursor(next_page_params)
        pagination = PaginationInfo(
            next_call=NextCallInfo(
                tool_name="some_tool",
                params={"chain_id": chain_id, "address": address, "cursor": next_cursor},
            )
        )

    # 5. Construct and return the final response
    return build_tool_response(
        data=main_data,
        notes=notes,
        pagination=pagination,
    ).model_dump()  # Return as a dictionary
```
### Performance Optimization

#### Concurrent API Calls

**When to Use:** When your tool needs data from multiple independent API sources (e.g., Blockscout + Metadata, block + transactions).

**Implementation Pattern:**

```python
import asyncio

# Execute multiple API calls concurrently
result1, result2 = await asyncio.gather(
    make_blockscout_request(base_url=base_url, api_path="/api/v2/addresses/{address}"),
    make_metadata_request(api_path="/api/v1/metadata", params={"addresses": address}),
    return_exceptions=True  # Critical: prevents one failure from breaking the entire operation
)

# Handle results with proper exception checking
if isinstance(result1, Exception):
    return f"Error fetching primary data: {result1}"

output_parts = ["Primary data:", json.dumps(result1)]

# Handle secondary data gracefully
if not isinstance(result2, Exception) and result2:
    output_parts.append("\nSecondary data:")
    output_parts.append(json.dumps(result2))

return "\n".join(output_parts)
```

**Key Points:**
- Always use `return_exceptions=True`

#### 8. Truncating Large and Nested Data Fields to Save Context (`return_type: str`)

**Rationale:** Some API fields, like the raw `data` field or deeply nested values inside a log's `decoded` object, can be extremely large and consume excessive LLM context. We shorten these values and explicitly flag the truncation, guiding the agent on how to retrieve the full data if needed.

**Implementation Pattern:**
This pattern uses a shared helper to centralize the truncation logic and conditionally adds an instructional note to the output.

```python
from .common import _process_and_truncate_log_items # Shared helper handles both raw and decoded truncation

async def tool_with_large_data_fields(chain_id: str, hash: str, ctx: Context) -> str:
    # 1. Get raw response from API
    base_url = await get_blockscout_base_url(chain_id)
    raw_response = await make_blockscout_request(...)

    # 2. Use the helper to process items and check for truncation.
    #    It shortens the raw `data` field and any long strings inside
    #    the nested `decoded` structure.
    processed_items, was_truncated = _process_and_truncate_log_items(
        raw_response.get("items", [])
    )

    # 3. Prepare the main JSON body
    output_json = json.dumps({"items": processed_items})
    output_string = f"**Some Data:**\n{output_json}"

    # 4. Conditionally add the instructional note
    if was_truncated:
        note = f"""
----
**Note on Truncated Data:**
Some data was truncated. To get the full data, use:
`curl "{base_url}/api/v2/some_endpoint/{hash}"`
"""
        output_string += note

    return output_string
```

#### 9. Recursively Truncating Nested Data Structures (`return_type: str` or `dict`)

**Rationale:** Some API fields, like `decoded_input` in a transaction, can contain deeply nested lists and tuples with large data blobs. A simple check is not enough. To handle this, we use a recursive helper to traverse the structure and truncate any long strings found within, replacing them with a structured object to signal the truncation.

**Implementation Pattern:**
This pattern uses a shared recursive helper to centralize the logic and conditionally adds an instructional note to the output.

```python
from .common import _recursively_truncate_and_flag_long_strings # Fictional helper for this example

async def tool_with_nested_data(chain_id: str, hash: str, ctx: Context) -> str | dict:
    # 1. Get raw response from API
    raw_response = await make_blockscout_request(...)

    # 2. Use the recursive helper on the part of the response with nested data
    processed_params, params_truncated = _recursively_truncate_and_flag_long_strings(
        raw_response.get("decoded_input", {}).get("parameters", [])
    )
    if params_truncated:
        raw_response["decoded_input"]["parameters"] = processed_params

    # 3. Check if any truncation occurred and prepare final output
    if params_truncated:
        # Return as a string with a note
        output_json = json.dumps(raw_response)
        note = f"""
----
**Note on Truncated Data:**
Some nested data was truncated. To get the full data, use:
`curl "{base_url}/api/v2/some_endpoint/{hash}"`
"""
        return f"{output_json}{note}"
    else:
        # Return the dictionary directly
        return raw_response
```
