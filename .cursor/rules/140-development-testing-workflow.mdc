---
description: Development Testing Workflow applied when developing new features or modifying existing tools
globs: 
alwaysApply: false
---
# Development Testing Workflow

When developing new features or modifying existing tools, follow this testing workflow:

## 1. Run Unit Tests First

Always run the unit test suite before and after making changes:

```bash
# Run all tests to ensure nothing is broken
pytest

# Run specific tests related to your changes
pytest tests/tools/test_address_tools.py -v
```

## 2. Add Tests for New Features

When adding new tools or modifying existing ones:

1.  Create or update the corresponding test file in `tests/tools/`.
2.  Write tests covering success scenarios, error cases, and edge cases.
3.  Ensure all external API calls are properly mocked using `unittest.mock.patch` and `AsyncMock`.
4.  Follow the specific implementation patterns outlined below.

### Key Testing Patterns & Guidelines

#### A. Use the `mock_ctx` Fixture

A reusable `pytest` fixture named `mock_ctx` is defined in `tests/conftest.py`. This fixture provides a pre-configured mock of the MCP `Context` object with an `AsyncMock` for `report_progress`.

**DO NOT** create a manual `MagicMock` for the context within your test functions.

**Correct Usage:**
```python
import pytest

@pytest.mark.asyncio
async def test_some_tool_success(mock_ctx):  # Request the fixture as an argument
    # ARRANGE
    # The mock_ctx object is ready to use.
    
    # ACT
    result = await some_tool(..., ctx=mock_ctx)

    # ASSERT
    # You can now make assertions on the fixture
    assert mock_ctx.report_progress.call_count > 0
```

#### B. Asserting on JSON within Formatted Strings

For tools that return a formatted string containing a JSON object (e.g., `get_address_logs`), **DO NOT** parse the JSON from the final string result in your test. This is brittle. Instead, **mock the serialization function (`json.dumps`)** to verify that the correct Python dictionary was passed to it *before* it was serialized.

However, the approach depends on the complexity of the tool.

##### The Simple Case: One Serialization Call

If the tool under test is the only function calling `json.dumps`, the approach is straightforward.

**Correct Usage:**
```python
# In a test for a tool like get_transaction_logs
from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_get_transaction_logs_correctly_prepares_json(mock_ctx):
    # ... (Arrange mocks for API calls)
    mock_api_response = {"items": [...]} 

    # Patch json.dumps where it is used in the tool's module
    with patch('blockscout_mcp_server.tools.transaction_tools.json.dumps') as mock_json_dumps:
        mock_json_dumps.return_value = '{"fake_json": true}'  # Return value doesn't matter

        # ACT
        result = await get_transaction_logs(..., ctx=mock_ctx)

        # ASSERT
        # Verify that json.dumps was called with the raw, unprocessed API response
        mock_json_dumps.assert_called_once_with(mock_api_response, indent=2)
```

##### The Complex Case: Multiple Serialization Calls

A more complex situation arises when the tool under test (e.g., `get_address_logs`) calls `json.dumps` for its main body, but it *also* calls a helper function (e.g., `encode_cursor`) which has its own internal call to `json.dumps`. A simple patch on `json.dumps` would incorrectly capture both calls, causing the test to fail.

The solution is to **mock both the low-level primitive (`json.dumps`) and the higher-level helper (`encode_cursor`)**. This isolates the test's focus, allowing you to verify each responsibility of the tool independently.

**Correct Usage (Advanced):**
```python
# In a test for a tool like get_address_logs
from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_get_address_logs_with_pagination(mock_ctx):
    # ARRANGE
    mock_api_response = {
        "items": [...],
        "next_page_params": {"block_number": 123, ...}
    }
    fake_cursor = "ENCODED_CURSOR_FROM_TEST"
    fake_json_body = '{"fake_json_body": true}'

    # THE FIX: Patch both `json.dumps` AND the helper `encode_cursor`
    with patch('blockscout_mcp_server.tools.address_tools.json.dumps') as mock_json_dumps, \
         patch('blockscout_mcp_server.tools.address_tools.encode_cursor') as mock_encode_cursor:

        # Configure both mocks
        mock_json_dumps.return_value = fake_json_body
        mock_encode_cursor.return_value = fake_cursor

        # ACT
        result = await get_address_logs(..., ctx=mock_ctx)

        # ASSERT
        # 1. Verify the call to `json.dumps` for the main body.
        #    This works because the call from `encode_cursor` is prevented by its mock.
        mock_json_dumps.assert_called_once_with(mock_api_response, indent=2)

        # 2. Verify the call to the helper function for the pagination part.
        mock_encode_cursor.assert_called_once_with(mock_api_response["next_page_params"])

        # 3. Verify the final string was assembled correctly from the outputs of both mocks.
        assert fake_json_body in result
        assert f'cursor="{fake_cursor}"' in result
```

#### C. Handling Repetitive Data in Assertions (DAMP vs. DRY)

When testing tools that transform a list of items (e.g., `lookup_token_by_symbol`), explicitly writing out the entire `expected_result` can lead to large, repetitive, and hard-to-maintain test code.

In these cases, it is better to **programmatically generate the `expected_result`** from the `mock_api_response`. This keeps the test maintainable while still explicitly documenting the transformation logic itself.

**Correct Usage:**
```python
import copy

@pytest.mark.asyncio
async def test_lookup_token_by_symbol_success(mock_ctx):
    # ARRANGE
    mock_api_response = {
        "items": [
            {"address_hash": "0xabc...", "name": "Token A"},
            {"address_hash": "0xdef...", "name": "Token B"}
        ]
    }

    # Generate the expected result programmatically
    expected_result = []
    for item in mock_api_response["items"]:
        new_item = copy.deepcopy(item)
        
        # Explicitly document the transformation logic
        new_item["address"] = new_item.pop("address_hash")
        new_item["token_type"] = "" # Add default fields
        
        expected_result.append(new_item)

    # ... (patching and ACT phase)

    # ASSERT
    # The final assertion is clean and compares the entire transformed structure.
    assert result == expected_result
```

#### D. General Assertions
-   **Progress Tracking:** Always verify the number of calls to `mock_ctx.report_progress` to ensure the user is kept informed.
-   **API Calls:** Assert that the mocked API helper functions (`make_blockscout_request`, etc.) are called exactly once with the correct `api_path` and `params`.
-   **Wrapper Integration:** For tools using `make_request_with_periodic_progress`, mock the wrapper itself and assert that it was called with the correct arguments (`request_function`, `request_args`, etc.).

## 3. Check Test Coverage

Ensure your changes are well-tested:

```bash
pytest --cov=blockscout_mcp_server --cov-report=term-missing
```

## 4. Run Integration Tests (Optional but Recommended)

Before final validation, run the integration test suite to ensure your changes have not broken our interaction with live APIs. This is especially important if you have modified any of the helper functions in `tools/common.py`.

```bash
pytest -m integration
```

## 5. End-to-End Validation

After unit tests pass, validate the changes using HTTP mode testing:

```bash
# Start the server
python -m blockscout_mcp_server --http

# Test your specific changes using curl commands
# (See examples in the End-to-End HTTP Testing section in [TESTING.md](mdc:TESTING.md))
```

## Continuous Integration Requirements

The test suite is designed to run in CI environments with:

- **No external dependencies**: All API calls are mocked.
- **File Size Limitation**: Test files must not exceed 500 LOC. If a file approaches this limit, split tests into multiple files (e.g., `test_some_tools_1.py`, `test_some_tools_2.py`) to maintain readability and focus on individual tools. This aligns with the naming pattern in `test_address_tools.py` and `test_address_tools_2.py`.
- **Deterministic results**: Tests produce consistent results across environments.
- **Clear failure reporting**: Failed tests provide detailed error information.

---
## Guidelines for Writing Integration Tests

While unit tests focus on logic with mocked data, integration tests verify our interaction with live external APIs. We have two distinct categories of integration tests, each with a specific purpose.

### Category 1: Helper-Level Integration Tests (Connectivity & Basic Contract)

These tests target the low-level helper functions in `tools/common.py` (e.g., `make_blockscout_request`, `get_blockscout_base_url`).

*   **Purpose:** To verify basic network connectivity and ensure the fundamental HTTP request/response cycle with each external service is working.
*   **Location:** `tests/integration/test_common_helpers.py`.
*   **What to Assert:**
    *   The request was successful (no HTTP errors).
    *   The top-level structure of the response is as expected (e.g., `isinstance(response, list)`).
    *   Presence of a few key, stable fields to confirm we're hitting the right endpoint.

### Category 2: Tool-Level Integration Tests (Data Extraction & Schema Validation)

These tests target the high-level MCP tool functions themselves (e.g., `get_latest_block`, `get_tokens_by_address`).

*   **Purpose:** To validate the "contract" between our tool's data processing logic and the live API's response schema. They ensure that the fields our tools *extract and transform* are still present and correctly structured in the live data. This protects against breaking changes in the API schema.
*   **Location:** `tests/integration/test_*_integration.py` (e.g., `test_block_tools_integration.py`).
*   **What to Call:** The actual MCP tool function (e.g., `await get_latest_block(chain_id="1", ctx=mock_ctx)`).
*   **What to Assert:**
    *   Focus on the **final, processed result** returned by the tool.
    *   Verify that the extracted data has the correct type and format (e.g., `assert isinstance(result["block_number"], int)`).
    *   For lists, check that items in the list contain the expected processed fields (e.g., `assert "address" in item`).
    *   For tools with string formatting (like pagination hints), assert that the key substrings are present in the final output.
*   **What to Avoid:** Do not re-test complex formatting logic already covered by unit tests. The focus is on verifying the *data extraction* was successful.

### General Rules for All Integration Tests

*   **Use Stable Targets:** Always test against non-volatile data points (e.g., a famous historical block, a well-known ENS name).
*   **Use the `integration` Marker:** Every integration test function **must** be decorated with `@pytest.mark.integration`.
