---
description: Development Testing Workflow applied when developing new features or modifying existing tools
globs: 
alwaysApply: false
---
# Development Testing Workflow

When developing new features or modifying existing tools, follow this testing workflow:

## 1. Run Unit Tests First

Always run the unit test suite before and after making changes:

```bash
# Run all tests to ensure nothing is broken
pytest

# Run specific tests related to your changes
pytest tests/tools/test_address_tools.py -v
```

## 2. Add Tests for New Features

When adding new tools or modifying existing ones:

1.  Create or update the corresponding test file in `tests/tools/`.
2.  Write tests covering success scenarios, error cases, and edge cases.
3.  Ensure all external API calls are properly mocked using `unittest.mock.patch` and `AsyncMock`.
4.  Follow the specific implementation patterns outlined below.

### Key Testing Patterns & Guidelines

#### A. Use the `mock_ctx` Fixture

A reusable `pytest` fixture named `mock_ctx` is defined in `tests/conftest.py`. This fixture provides a pre-configured mock of the MCP `Context` object with an `AsyncMock` for `report_progress`.

**DO NOT** create a manual `MagicMock` for the context within your test functions.

**Correct Usage:**
```python
import pytest

@pytest.mark.asyncio
async def test_some_tool_success(mock_ctx):  # Request the fixture as an argument
    # ARRANGE
    # The mock_ctx object is ready to use.
    
    # ACT
    result = await some_tool(..., ctx=mock_ctx)

    # ASSERT
    # You can now make assertions on the fixture
    assert mock_ctx.report_progress.call_count > 0
```

#### B. Asserting on JSON within Formatted Strings

For tools that return a formatted string containing a JSON object (e.g., `get_address_logs`, `get_transaction_logs`), **DO NOT** parse the JSON from the final string result in your test. This is brittle.

Instead, **mock `json.dumps`** to verify that the correct Python dictionary was passed to it *before* it was serialized into a string.

**Correct Usage:**
```python
# In a test for a tool like get_address_logs
from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_get_address_logs_correctly_prepares_json(mock_ctx):
    # ... (Arrange mocks for API calls)
    mock_api_response = {"items": [...]}

    # Patch json.dumps where it is used in the tool's module
    with patch('blockscout_mcp_server.tools.address_tools.json.dumps') as mock_json_dumps:
        mock_json_dumps.return_value = '{"fake_json": true}' # Return value doesn't matter

        # ACT
        result = await get_address_logs(..., ctx=mock_ctx)

        # ASSERT
        # Verify that json.dumps was called with the raw, unprocessed API response
        mock_json_dumps.assert_called_once_with(mock_api_response, indent=2)

        # You can still assert on other parts of the string result, like pagination hints
        assert "To get the next page" in result
```

#### C. Handling Repetitive Data in Assertions (DAMP vs. DRY)

When testing tools that transform a list of items (e.g., `lookup_token_by_symbol`), explicitly writing out the entire `expected_result` can lead to large, repetitive, and hard-to-maintain test code.

In these cases, it is better to **programmatically generate the `expected_result`** from the `mock_api_response`. This keeps the test maintainable while still explicitly documenting the transformation logic itself.

**Correct Usage:**
```python
import copy

@pytest.mark.asyncio
async def test_lookup_token_by_symbol_success(mock_ctx):
    # ARRANGE
    mock_api_response = {
        "items": [
            {"address_hash": "0xabc...", "name": "Token A"},
            {"address_hash": "0xdef...", "name": "Token B"}
        ]
    }

    # Generate the expected result programmatically
    expected_result = []
    for item in mock_api_response["items"]:
        new_item = copy.deepcopy(item)
        
        # Explicitly document the transformation logic
        new_item["address"] = new_item.pop("address_hash")
        new_item["token_type"] = "" # Add default fields
        
        expected_result.append(new_item)

    # ... (patching and ACT phase)

    # ASSERT
    # The final assertion is clean and compares the entire transformed structure.
    assert result == expected_result
```

#### D. General Assertions
-   **Progress Tracking:** Always verify the number of calls to `mock_ctx.report_progress` to ensure the user is kept informed.
-   **API Calls:** Assert that the mocked API helper functions (`make_blockscout_request`, etc.) are called exactly once with the correct `api_path` and `params`.
-   **Wrapper Integration:** For tools using `make_request_with_periodic_progress`, mock the wrapper itself and assert that it was called with the correct arguments (`request_function`, `request_args`, etc.).

## 3. Check Test Coverage

Ensure your changes are well-tested:

```bash
pytest --cov=blockscout_mcp_server --cov-report=term-missing
```

## 4. Run Integration Tests (Optional but Recommended)

Before final validation, run the integration test suite to ensure your changes have not broken our interaction with live APIs. This is especially important if you have modified any of the helper functions in `tools/common.py`.

```bash
pytest -m integration
```

## 5. End-to-End Validation

After unit tests pass, validate the changes using HTTP mode testing:

```bash
# Start the server
python -m blockscout_mcp_server --http

# Test your specific changes using curl commands
# (See examples in the End-to-End HTTP Testing section in [TESTING.md](mdc:TESTING.md))
```

## Continuous Integration Requirements

The test suite is designed to run in CI environments with:

- **No external dependencies**: All API calls are mocked.
- **File Size Limitation**: Test files must not exceed 500 LOC. If a file approaches this limit, split tests into multiple files (e.g., `test_some_tools_1.py`, `test_some_tools_2.py`) to maintain readability and focus on individual tools. This aligns with the naming pattern in `test_address_tools.py` and `test_address_tools_2.py`.
- **Deterministic results**: Tests produce consistent results across environments.
- **Clear failure reporting**: Failed tests provide detailed error information.

---
## Guidelines for Writing Integration Tests

While the unit test guidelines focus on mocking, integration tests require a different approach focused on stability and precision.

#### 1. Test the Boundary, Not the Tool Logic
-   Integration tests should primarily target the helper functions in `tools/common.py` that make the actual network calls (`make_blockscout_request`, etc.).
-   Avoid re-testing the complex formatting logic of a full tool. If the helper function returns the correct data from the live API, we can trust the unit-tested tool to format it correctly.

#### 2. Use Stable and Non-Volatile Targets
-   To ensure tests are reliable and deterministic, always test against stable data points that are unlikely to change.
-   **Good examples:** A famous historical block (`19000000` on Ethereum), a well-known ENS name (`vitalik.eth`), or a genesis block.
-   **Bad examples:** The *latest* block, a recent transaction, or a token with a fluctuating market cap.

#### 3. Assert on Structure and Stable Values
-   When checking a response from a live API, prioritize asserting the *structure* of the data (e.g., `assert "timestamp" in response_data`) and the *type* of the data (`assert isinstance(response_data["height"], int)`).
-   Only assert on specific *values* when they are guaranteed to be stable, like the block number you requested or the known address for an ENS name.

#### 4. Always Use the `integration` Marker
-   Every integration test function **must** be decorated with `@pytest.mark.integration`. This is critical for separating the test suites and ensuring the default `pytest` run remains fast and offline.
